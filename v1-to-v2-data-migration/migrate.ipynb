{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cff59fc8",
   "metadata": {},
   "source": [
    "### Authenticate\n",
    "Get the client id and secret from env vars for prod.\n",
    "In dev we can create a new client each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6a04ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T16:16:51.407345Z",
     "iopub.status.busy": "2025-08-11T16:16:51.407194Z",
     "iopub.status.idle": "2025-08-11T16:16:51.591949Z",
     "shell.execute_reply": "2025-08-11T16:16:51.585965Z"
    }
   },
   "outputs": [],
   "source": [
    "import { registerSystem } from './helpers/gqlHandlers.ts'\n",
    "import { authenticate, getTokenForSystemClient } from './helpers/authentication.ts'\n",
    "import { CLIENT_ID, CLIENT_SECRET } from './helpers/vars.ts'\n",
    "import { ADMIN_USERNAME, ADMIN_PASSWORD } from './helpers/vars.ts'\n",
    "\n",
    "\n",
    "let clientId: string | null = null\n",
    "let clientSecret: string | null = null\n",
    "\n",
    "if (CLIENT_ID && CLIENT_SECRET) {\n",
    "  clientId = CLIENT_ID\n",
    "  clientSecret = CLIENT_SECRET\n",
    "} else {\n",
    "  // For dev mode\n",
    "  const adminToken = await authenticate(ADMIN_USERNAME, ADMIN_PASSWORD)\n",
    "  const systemRegistration = await registerSystem(adminToken)\n",
    "\n",
    "  clientId = systemRegistration.data.registerSystem.system.clientId\n",
    "  clientSecret = systemRegistration.data.registerSystem.clientSecret\n",
    "}\n",
    "\n",
    "const sysToken = await getTokenForSystemClient(clientId, clientSecret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a23bc8",
   "metadata": {},
   "source": [
    "### Sync locations\n",
    "\n",
    "On update to v1.9, the postgres db won't have locations so this is required to sync them from mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd611e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { syncLocations } from './helpers/gqlHandlers.ts'\n",
    "\n",
    "const res = await syncLocations(sysToken)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a83c37d",
   "metadata": {},
   "source": [
    "### Fetch birth and death events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e3dedc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T16:16:51.834334Z",
     "iopub.status.busy": "2025-08-11T16:16:51.834216Z",
     "iopub.status.idle": "2025-08-11T16:16:51.858997Z",
     "shell.execute_reply": "2025-08-11T16:16:51.852701Z"
    }
   },
   "outputs": [],
   "source": [
    "import { fetchEvents } from './helpers/formsHandlers.ts'\n",
    "import { extractFieldType } from './helpers/utils.ts'\n",
    "\n",
    "const events = await fetchEvents(sysToken)\n",
    "\n",
    "const ignoredFields = ['DIVIDER', 'PARAGRAPH']\n",
    "\n",
    "const birthEvent = events.find((x) => x.id === 'birth')\n",
    "const deathEvent = events.find((x) => x.id === 'death')\n",
    "\n",
    "const birthEventFields = new Set(\n",
    "  extractFieldType(birthEvent, 'fields')\n",
    "    .filter((x) => !ignoredFields.includes(x.type))\n",
    "    .map((f) => f.id)\n",
    "    .filter((x) => x)\n",
    ")\n",
    "\n",
    "const deathEventFields = new Set(\n",
    "  extractFieldType(deathEvent, 'fields')\n",
    "    .filter((x) => !ignoredFields.includes(x.type))\n",
    "    .map((f) => f.id)\n",
    "    .filter((x) => x)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6262bef",
   "metadata": {},
   "source": [
    "### Get all potential resolvers\n",
    "Use only resolvers for used event fields to avoid nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cfffb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T16:16:51.859822Z",
     "iopub.status.busy": "2025-08-11T16:16:51.859675Z",
     "iopub.status.idle": "2025-08-11T16:16:51.868168Z",
     "shell.execute_reply": "2025-08-11T16:16:51.861669Z"
    }
   },
   "outputs": [],
   "source": [
    "import defaultResolvers, {\n",
    "  defaultBirthResolver,\n",
    "  defaultDeathResolver,\n",
    "} from './helpers/defaultResolvers.ts'\n",
    "import { countryResolver } from './countryData/countryResolvers.ts'\n",
    "\n",
    "const allResolvers = { ...defaultResolvers, ...countryResolver }\n",
    "\n",
    "const birthResolver = Object.fromEntries(\n",
    "  Object.entries({...defaultBirthResolver, ...allResolvers}).filter(([key, _value]) =>\n",
    "    [...birthEventFields].includes(key)\n",
    "  )\n",
    ")\n",
    "\n",
    "const deathResolver = Object.fromEntries(\n",
    "  Object.entries({...defaultDeathResolver, ...allResolvers}).filter(([key, _value]) =>\n",
    "    [...deathEventFields].includes(key)\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7467bae6",
   "metadata": {},
   "source": [
    "### Migrate births"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459305da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T16:16:51.869114Z",
     "iopub.status.busy": "2025-08-11T16:16:51.868983Z",
     "iopub.status.idle": "2025-08-11T16:16:52.243480Z",
     "shell.execute_reply": "2025-08-11T16:16:52.237813Z"
    }
   },
   "outputs": [],
   "source": [
    "import { EVENT } from './helpers/vars.ts'\n",
    "import {\n",
    "  fetchBirthRegistration,\n",
    "  bulkImport,\n",
    "  fetchAllBirthRegistrations\n",
    "} from './helpers/gqlHandlers.ts'\n",
    "import { transform } from './helpers/transform.ts'\n",
    "import { batch, getIndexErrors } from './helpers/utils.ts'\n",
    "import { RECORD_SKIP, SINGLE_RECORD } from './helpers/vars.ts'\n",
    "\n",
    "const individualFailures = []\n",
    "\n",
    "const migrateBirth = async (entryIds) => {\n",
    "  const transformed = []\n",
    "\n",
    "  for (const entryId of entryIds) {\n",
    "    // Get entry\n",
    "    const birthRegistrationData = await fetchBirthRegistration(\n",
    "      entryId,\n",
    "      sysToken\n",
    "    )\n",
    "    if (!birthRegistrationData.data.fetchBirthRegistration) {\n",
    "      console.error(JSON.stringify(birthRegistrationData, null, 2))\n",
    "      throw new Error(`No data for ${entryId}`)\n",
    "    }\n",
    "\n",
    "    let document\n",
    "    try {\n",
    "      document = transform(\n",
    "        birthRegistrationData.data.fetchBirthRegistration,\n",
    "        birthResolver,\n",
    "        'birth'\n",
    "      )\n",
    "      transformed.push(document)\n",
    "    } catch (err) {\n",
    "      // TODO remove for PROD\n",
    "      console.log('document')\n",
    "      console.log(JSON.stringify(document, null, 2))\n",
    "      console.log('birthRegistrationData')\n",
    "      console.log(JSON.stringify(birthRegistrationData))\n",
    "      individualFailures.push(entryId)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return await bulkImport(transformed, sysToken)\n",
    "}\n",
    "\n",
    "if (EVENT === 'birth' && !SINGLE_RECORD) {\n",
    "  const skippedPages = RECORD_SKIP ? Math.floor(RECORD_SKIP / 1000) : 0\n",
    "  const pageSize = 1000\n",
    "  const batchSize = 100\n",
    "  let itemsRemaining = 0\n",
    "  let page = 1 + skippedPages\n",
    "  let totalProcessed = skippedPages * pageSize\n",
    "\n",
    "  if (RECORD_SKIP) {\n",
    "    console.log(`Skipping first ${skippedPages * pageSize} records`)\n",
    "  }\n",
    "  do {\n",
    "    const birthRegistrations = await fetchAllBirthRegistrations(\n",
    "      sysToken,\n",
    "      page,\n",
    "      pageSize\n",
    "    )\n",
    "    if (!birthRegistrations.data.searchEvents) {\n",
    "      console.error(JSON.stringify(birthRegistrations, null, 2))\n",
    "      throw new Error('No data from searchEvents')\n",
    "    }\n",
    "\n",
    "    const { results, totalItems } = birthRegistrations.data.searchEvents\n",
    "    const birthIds = results.map((x) => x.id)\n",
    "\n",
    "    console.log(\n",
    "      `Processing next page of ${birthIds.length} of ${totalItems} total records`\n",
    "    )\n",
    "\n",
    "    const batches = batch(birthIds, batchSize)\n",
    "\n",
    "    for (const batch of batches) {\n",
    "      const indexResult = await migrateBirth(batch)\n",
    "      console.log(`  - Processed batch of ${batch.length} records`)\n",
    "      const errors = getIndexErrors(indexResult)\n",
    "      if (errors) {\n",
    "        console.error('Errors during bulk import', errors)\n",
    "        throw new Error(errors)\n",
    "      }\n",
    "    }\n",
    "\n",
    "    totalProcessed += birthIds.length\n",
    "    itemsRemaining = Math.max(0, totalItems - totalProcessed)\n",
    "\n",
    "    console.log(\n",
    "      `Processed ${totalProcessed} of ${totalItems} birth registrations... with ${itemsRemaining} remaining.`\n",
    "    )\n",
    "    page += 1\n",
    "  } while (itemsRemaining > 0)\n",
    "} else if (EVENT === 'birth' && SINGLE_RECORD) {\n",
    "  const birthRegistrationData = await fetchBirthRegistration(SINGLE_RECORD, sysToken)\n",
    "  if (!birthRegistrationData.data.fetchBirthRegistration) {\n",
    "    console.error(JSON.stringify(birthRegistrationData, null, 2))\n",
    "    throw new Error(`No data for ${SINGLE_RECORD}`)\n",
    "  }\n",
    "  \n",
    "  const indexResult = await migrateBirth([SINGLE_RECORD])\n",
    "  const errors = getIndexErrors(indexResult)\n",
    "  if (errors) {\n",
    "    console.error('Errors during bulk import', errors)\n",
    "    throw new Error(errors)\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaabe49",
   "metadata": {},
   "source": [
    "### Migrate Deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641f1678",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T16:16:52.244617Z",
     "iopub.status.busy": "2025-08-11T16:16:52.244348Z",
     "iopub.status.idle": "2025-08-11T16:16:52.251327Z",
     "shell.execute_reply": "2025-08-11T16:16:52.245101Z"
    }
   },
   "outputs": [],
   "source": [
    "import {\n",
    "  fetchDeathRegistration,\n",
    "  fetchAllDeathRegistrations,\n",
    "} from './helpers/gqlHandlers.ts'\n",
    "\n",
    "const migrateDeath = async (entryIds) => {\n",
    "  const transformed = []\n",
    "\n",
    "  for (const entryId of entryIds) {\n",
    "    // Get entry\n",
    "    const deathRegistrationData = await fetchDeathRegistration(\n",
    "      entryId,\n",
    "      sysToken\n",
    "    )\n",
    "    if (!deathRegistrationData.data.fetchDeathRegistration) {\n",
    "      console.error(JSON.stringify(deathRegistrationData, null, 2))\n",
    "      throw new Error(`No data for ${entryId}`)\n",
    "    }\n",
    "\n",
    "    let document\n",
    "    try {\n",
    "      document = transform(\n",
    "        deathRegistrationData.data.fetchDeathRegistration,\n",
    "        deathResolver,\n",
    "        'death'\n",
    "      )\n",
    "      transformed.push(document)\n",
    "    } catch (err) {\n",
    "      // TODO remove for PROD\n",
    "      console.log('document')\n",
    "      console.log(JSON.stringify(document, null, 2))\n",
    "      console.log('deathRegistrationData')\n",
    "      console.log(JSON.stringify(deathRegistrationData))\n",
    "      individualFailures.push(entryId)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  return await bulkImport(transformed, sysToken)\n",
    "}\n",
    "\n",
    "if (EVENT === 'death' && !SINGLE_RECORD) {\n",
    "  const skippedPages = RECORD_SKIP ? Math.floor(RECORD_SKIP / 1000) : 0\n",
    "  const pageSize = 1000\n",
    "  const batchSize = 100\n",
    "  let itemsRemaining = 0\n",
    "  let page = 1 + skippedPages\n",
    "  let totalProcessed = skippedPages * pageSize\n",
    "\n",
    "  if (RECORD_SKIP) {\n",
    "    console.log(`Skipping first ${skippedPages * pageSize} records`)\n",
    "  }\n",
    "\n",
    "  do {\n",
    "    const deathRegistrations = await fetchAllDeathRegistrations(\n",
    "      sysToken,\n",
    "      page,\n",
    "      pageSize\n",
    "    )\n",
    "    if (!deathRegistrations.data.searchEvents) {\n",
    "      console.error(JSON.stringify(deathRegistrations, null, 2))\n",
    "      throw new Error('No data from searchEvents')\n",
    "    }\n",
    "    const { results, totalItems } = deathRegistrations.data.searchEvents\n",
    "    const deathIds = results.map((x) => x.id)\n",
    "    console.log(\n",
    "      `Processing next page of ${deathIds.length} of ${totalItems} total records`\n",
    "    )\n",
    "\n",
    "    const batches = batch(deathIds, batchSize)\n",
    "\n",
    "    for (const batch of batches) {\n",
    "      const indexResult = await migrateDeath(batch)\n",
    "      console.log(`  - Processed batch of ${batch.length} records`)\n",
    "      const errors = getIndexErrors(indexResult)\n",
    "      if (errors) {\n",
    "        console.error('Errors during bulk import', errors)\n",
    "        throw new Error(errors)\n",
    "      }\n",
    "    }\n",
    "\n",
    "    totalProcessed += deathIds.length\n",
    "    itemsRemaining = Math.max(0, totalItems - totalProcessed)\n",
    "\n",
    "    console.log(\n",
    "      `Processed ${totalProcessed} of ${totalItems} death registrations... with ${itemsRemaining} remaining.`\n",
    "    )\n",
    "    page += 1\n",
    "  } while (itemsRemaining > 0)\n",
    "} else if (EVENT === 'death' && SINGLE_RECORD) {\n",
    "  const deathRegistrationData = await fetchDeathRegistration(SINGLE_RECORD, sysToken)\n",
    "  if (!deathRegistrationData.data.fetchDeathRegistration) {\n",
    "    console.error(JSON.stringify(deathRegistrationData, null, 2))\n",
    "    throw new Error(`No data for ${SINGLE_RECORD}`)\n",
    "  }\n",
    "  \n",
    "  const indexResult = await migrateDeath([SINGLE_RECORD])\n",
    "  const errors = getIndexErrors(indexResult)\n",
    "  if (errors) {\n",
    "    console.error('Errors during bulk import', errors)\n",
    "    throw new Error(errors)\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c304f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "console.log('Migration completed.')\n",
    "if (individualFailures.length > 0) {\n",
    "  console.log(\n",
    "    `However, there were ${individualFailures.length} individual records that failed to migrate:`\n",
    "  )\n",
    "  console.log(individualFailures)\n",
    "} else {\n",
    "  console.log('All individual records migrated successfully.')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2cd5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import { reindex } from \"./helpers/gqlHandlers.ts\";\n",
    "\n",
    "const reindexResponse = await reindex(sysToken);\n",
    "reindexResponse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba9968f",
   "metadata": {},
   "source": [
    "### Output results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6274c73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T16:16:52.252559Z",
     "iopub.status.busy": "2025-08-11T16:16:52.252321Z",
     "iopub.status.idle": "2025-08-11T16:16:52.259207Z",
     "shell.execute_reply": "2025-08-11T16:16:52.253589Z"
    }
   },
   "outputs": [],
   "source": [
    "console.log('üçû Declarations succesfully migrated:')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "codemirror_mode": "typescript",
   "file_extension": ".ts",
   "mimetype": "text/x.typescript",
   "name": "typescript",
   "nbconvert_exporter": "script",
   "pygments_lexer": "typescript",
   "version": "5.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
